<!-- Copyright by Vitali Fedulov (www.similar.pictures) 2015-2018 -->

<!DOCTYPE html>
<html lang="en">

<head>
  <title>Algorithm for perceptual image comparison</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/png" href="images/favicon.png">
  <link rel="stylesheet" type="text/css" href="css/index.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">
  <script src="js/jquery.min.js"></script>
</head>




<body>

  <!-- Header  -->
  <div class="header">
      <a href = "index.html" class="logo"><h1>Similar<span class="logo-dot">.</span>Pictures</h1></a>
      <h4 class="logo-statement">Find near duplicates on your computer</h4>
  </div>
  
  <div class="nav">
      <!-- Set manually class "active" for corresponding nav-tab. -->
      <div class="nav-tab"><a href="index.html">Home</a></div>
      <div class="nav-tab"><a href="duplicate-image-search-example.html">Screenshot</a></div>
      <div class="nav-tab"><a href="image-analysis-and-view-tools.html">Tools</a></div>
      <div class="nav-tab active"><a href="algorithm-for-perceptual-image-comparison.html">Algorithm</a></div>
      <div class="nav-tab"><a href="about.html">About</a></div>
      <div class="line"></div> <!-- Gray line placeholder. Should stay here. -->
  </div>


  <!-- Blog organization:
    1. Home page navigation links to the latest blog-[date].html file. The pages must never
       change file name/address, so if linked by other sites they stay constant.
    2. Each blog page will later have a list of all articles linked by html injection,
       e.g. secondary navigation menu. Place in upper part of the blog as TOC.
  -->

  <!-- Contents -->

  <p class="published"><time datetime="2018-10-29">2018-10-29</time></p>
  <h3 id="2018-10-29">Algorithm for perceptual image comparison</h3>  <!-- Id used for URL anchors.-->
    
    <p>The image comparison algorithm I developed for the service makes use of perceptual similarity by performing the following set of operations:</p>


    <h4>1. Mask generation</h4>

    <p>A set of square masks is generated. A mask is a black square image with several white pixels aggregating locally. Such pixel groups are located in distinct positions from mask to mask. The white pixels define image sub-regions to calculate average color at each of the regions. Such color values will be used during comparison stage. Depending on implementation the number of masks is 300-500, and the mask size from 8x8 to 24x24 pixels. A single mask size is used in specific implementation.</p>

    <figure class="blog-img-1">
      <img src="images/2018-10-29-image-masks-for-hash-generation.png" alt="masks for image comparison">
      <figcaption>Example of 16x16 masks with one of them shown at larger scale</figcaption>
    </figure>

    <p>In the latest implementation, instead of having random shapes, mask sub-regions have regular 3x3 shape (median filter).</p>

    <h4>2. Image resizing</h4>
    
    <p>Input images are resized to the mask size. Resampling quality should be high enough in order to preserve near exact average colors in sub-regions of high-resolution input images. Low quality resampling would cause the algorithm to fail.</p>

    <figure class="blog-img-1">
      <img src="images/2018-10-29-image-resizing.png" alt="photo resizing for image clustering">
      <figcaption>Input images are resized to the mask size</figcaption>
    </figure>


    <h4>3. Generating image hashes</h4>

    <p>An image hash is an array of average color values defined by white pixels of separate masks superimposed over the resized image. Example hash: [23, 126, 77, ...], where the array length is equal to the number of masks. Each value in the array corresponds to an average color value corresponding to the location of white pixels in one mask. Black pixels of a mask are not used for calculations. Best results are achieved by mixing color channel values, so that each value corresponds to a different color channel or gray in the following order within a hash: [r, g, k, r, g, k, ...], where r is red, g is green, k is grayscale. Blue is not used as a separate value because it was found less relevant for perceptual similarity.</p>


    <figure class="blog-img-1">
      <img src="images/2018-10-29-applying-mask-to-image.png" alt="similar image hash">
      <figcaption>Calculating hash sub-value (average color in white-pixel area of the mask)</figcaption>
    </figure>


    <h4>4. Image to image comparison</h4>

    <p>An input to this operation is a pair of image hashes and original image sizes. Image sizes are used as a first step to quickly eliminate possible mismatch. If image proportions are considerably different, images are considered non-similar, so no further checks are performed.</p>
    
    <p>In the next step two image hashes are compared in a loop value-by-value (e.g. color by color). If a color difference for two images is larger than a threshold (~50), images are considered non-similar and the loop is broken. This allows to eliminate non-similar pictures before going to the more computationally intensive next step.</p>

    <p>In the last step two image hashes are compared by their cosine similarity. If the similarity is smaller than threshold (~0.97), the images are considered different. This is the final step of the comparison procedure.</p>

    <p>Image pairs that passed all the three filters above are similar (with high probability).</p>
    
    <h4>Possible optimizations</h4>

    <p><strong>Optimization 1:</strong> The algorithm benefits from histogram normalization applied to resized images. This allows to better compare similar images containing line drawings, where white background occupies the majority of space. Normalization approach will sometimes generate false positives, e.g. underexposed images normalized and found similar to well balanced images.</p>

    <p><strong>Optimization 2:</strong> If searching for very similar images, e.g. strictly resized images, one can decrease color threshold and increase cosine similarity; alternatively increase mask size. In general, assuming same size of white sub-regions of the masks, smaller masks allow better generalization, e.g. comparing overall color/brightness distribution over the images. But such approach will also generate more false positives.</p>

    <p><strong>Optimization 3:</strong> Resizing input images to a square mask causes proportion changes for non-square images and allows to preserve information from every region of an image. Such resizing is optional. Instead it is possible to use/resize only the central square area of the input image for comparison, thus discarding information outside the central square. This should not cause considerable loss in comparison precision.</p>

    <p><strong>Optimization 4a:</strong> In addition to color values for hash generation it is also possible to use filter-based values, e.g. by passing an image through an edge detector. This allows to count additional visual signals within mask sub-regions. For example a forest photo will have distinct edge values compared to some smooth background of the same color (because trees have leaves, thus many edges). Such approach allows to distinguish textures. The challenge is finding optimal coefficients between edge information and color information during the hash comparison step.</p>
  
    <p><strong>Optimization 4b:</strong> Instead of summation of colors within mask sub-regions, it is possible to sum over features extracted with convolutional neural network filter layers.</p>
    
    <p><strong>Optimization 5:</strong> In the algorithm above white pixels of the masks form small near circular regions. Instead it is possible to use more complex subject-specific region shapes. E.g. face-like masks to detect faces. Mask summation regions can also have gray-scale values instead of being pure white.</p>
 
    <h4>Resources</h4>

    <p>Algorithm in Github (Golang): <a href="https://github.com/vitali-fedulov/images">image comparison</a>.</p>



    


  <div class = "footer">Copyright &#169; 2018 Vitali Fedulov. All rights reserved.</div>
  
</body>
</html>

